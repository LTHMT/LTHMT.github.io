
## 1.爬虫基本概念
### 1.1 HTTP 请求 
    ```
    HTTP 请求是指客户端（通常是浏览器或爬虫）向服务器发送请求，以获取服务器上的资源。

    常见的 HTTP 请求方法包括 `GET`（获取资源）、`POST`（提交数据）、`PUT`（更新资源）和 `DELETE`（删除资源）等。

    通过发送 HTTP 请求，客户端可以获取网页的 HTML 文档并进一步处理和解析其中的数据。
    ```
### 1.2 HTML 文档
    ```
    HTML 文档是描述网页结构和内容的文件。HTML 由各种标签（如 `<html>`、`<head>`、`<body>` 等）和文本内容组成。

    这些标签用于定义网页的标题、段落、图片、链接等信息。浏览器解析 HTML 文档并呈现为用户可见的网页，而爬虫则解析 HTML 内容，以提取所需的数据。
    ```
### 1.3 网页爬取 
    ```
    网页爬取是指自动化程序从网页上获取数据的过程。爬虫通过发送 HTTP 请求获取 HTML 文档，并解析网页内容，
    从中提取出特定的信息，如标题、价格、评论等。通过网页爬取，可以高效地从大量网页中收集数据。
    ```

### 1.4 网页爬取的一般流程 (ウェブスクレイピングの一般的なプロセス)
    ```
    网页爬取通常包括以下几个步骤：

    - **网页获取**：向目标网站发送 HTTP 请求，获取 HTML 文档。

    - **网页解析**：使用解析工具（如正则表达式、XPath、CSS选择器）分析 HTML 文档，找到所需的内容位置。

    - **内容提取**：从解析后的内容中提取目标数据。

    - **数据存储**：将提取的数据存储到文件（如 CSV、JSON）或数据库中，以便后续分析和处理。

    通过以上流程，爬虫能够系统化地从大量网页中提取所需数据，便于进一步分析和利用。
    ```

    好的，我会基于文件中的内容创建一个中文的基础爬虫教程，重点介绍基础爬虫知识和 Selenium 的应用。以下是教程内容：


## 2. 基础爬虫工具：Requests 和 BeautifulSoup

在 Python 中，可以使用 `requests` 库发送 HTTP 请求，用 `BeautifulSoup` 库解析 HTML 文档。以下是一个简单示例：

```python
import requests
from bs4 import BeautifulSoup

# 发送请求
url = "https://example.com"
response = requests.get(url)

# 解析 HTML 内容
soup = BeautifulSoup(response.text, 'html.parser')

# 提取内容，例如标题
title = soup.find('title').text
print(f"网页标题: {title}")
```

## 3. 动态网页和 Selenium 的应用

一些网页内容是通过 JavaScript 动态加载的，传统的爬虫方法无法直接获取。为此，我们可以使用 `Selenium`，这是一个自动化浏览器工具，可以模拟用户操作并执行 JavaScript 代码，从而加载网页中的动态内容。

### 3.1 安装 Selenium 和浏览器驱动

首先，安装 Selenium：
```bash
pip install selenium
```

然后，下载对应的浏览器驱动（如 ChromeDriver）并将其路径添加到系统环境变量中。

### 3.2 使用 Selenium 获取动态内容

以下是一个使用 Selenium 抓取动态内容的示例：

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
import time

# 初始化 Chrome 浏览器
driver = webdriver.Chrome()

# 打开网页
url = "https://example.com"
driver.get(url)

# 等待页面加载完成
time.sleep(3)

# 提取动态内容，例如获取带有特定类名的元素
elements = driver.find_elements(By.CLASS_NAME, "dynamic-content")
for element in elements:
    print(element.text)

# 关闭浏览器
driver.quit()
```

### 3.3 Selenium 操作网页元素

Selenium 提供了丰富的 API，可以实现模拟用户操作，如点击按钮、输入文本等。

- **输入文本**：
  ```python
  input_box = driver.find_element(By.ID, "search-box")
  input_box.send_keys("Selenium 爬虫")
  ```

- **点击按钮**：
  ```python
  search_button = driver.find_element(By.ID, "search-button")
  search_button.click()
  ```

- **滚动页面**（加载更多内容）：
  ```python
  driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
  ```

## 4. 爬虫最佳实践

- **避免过快请求**：添加适当的 `time.sleep()` 延迟，避免频繁请求服务器。
- **遵循 robots.txt**：查看网站的 `robots.txt` 文件，确保爬虫行为不违反网站规定。
- **错误处理**：使用 `try-except` 捕获错误，确保爬虫在遇到异常时不会崩溃。

## 5. 完整示例：爬取动态内容

以下是一个完整的示例，使用 Selenium 模拟用户搜索内容并抓取结果：

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
import time

# 初始化 Chrome 浏览器
driver = webdriver.Chrome()

try:
    # 打开目标网页
    url = "https://example.com/search"
    driver.get(url)

    # 输入搜索关键词
    search_box = driver.find_element(By.ID, "search-box")
    search_box.send_keys("Python 爬虫")

    # 点击搜索按钮
    search_button = driver.find_element(By.ID, "search-button")
    search_button.click()

    # 等待加载搜索结果
    time.sleep(3)

    # 提取结果
    results = driver.find_elements(By.CLASS_NAME, "result-title")
    for result in results:
        print(result.text)

finally:
    # 关闭浏览器
    driver.quit()
```

## 6. 数据存储

可以将爬取的数据存储为 CSV 或 JSON 文件：

```python
import csv

# 存储为 CSV
with open('data.csv', mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(['Title'])  # 表头
    for result in results:
        writer.writerow([result.text])
```

---

## 7.个人爬虫实操笔记
    此为没有加密的网站的动态爬虫

- 7.1 导入包
    ```python 
    from selenium import webdriver
    import requests
    from selenium.webdriver.common.by import By
    import os
    from pathlib import Path
    from tqdm import tqdm
    import time
    from tqdm.contrib import tenumerate,tzip,tmap
    ```

- 7.2 定义保存图片函数
    ```python
    def get_img(url, fn):
    # Get the user's home directory
        home_dir = str(Path.home())
    
    # Create the 'ssunbiki' directory in the home directory
        os.makedirs(os.path.join(home_dir, 'hana-bunny'), exist_ok=True)
    
    # Download the image and save it to the 'ssunbiki' directory
        resp = requests.get(url)
        with open(os.path.join(home_dir, 'hana-bunny', '{}.png'.format(fn)), 'wb') as f:
            f.write(resp.content)

    ```

- 7.3 配置webdriver
    ```python
    options=webdriver.ChromeOptions()
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--sandbox')
    #options.add_argument('--headless')
    ```

- 7.4 定义网站登录器
    ```python
    def login(url,user,password):
        driver=webdriver.Chrome(options=options)
        driver.get(url)
        driver.set_window_size(1920, 1080)
        #user=str(user)
        #password=str(password)
        username=driver.find_element(By.XPATH,value='//input[@type="text"]')
        passwordname=driver.find_element(By.XPATH,value='//input[@type="password"]')
        login2=driver.find_element(By.XPATH,value='//button[@type="submit"]')

        username.send_keys(user)
        passwordname.send_keys(password)
        time.sleep(1)
        login2.click()
        time.sleep(3)
        p=driver.find_element(By.XPATH, "//div[@class='sidebar_inner']").find_elements(By.TAG_NAME, 'a')
        web2=p[3].get_attribute('href')
        driver.get(web2)
        c=driver.find_elements(By.TAG_NAME,'a')
        for num,i in enumerate(c):
            if num>=30:
                h=i.get_attribute('href')
                print(num,h)
    ```

- 7.5 定义图片获取
    ```python
    def login(url,user,password):
        driver=webdriver.Chrome(options=options)
        driver.get(url)
        driver.set_window_size(1920, 1080)
        #user=str(user)
        #password=str(password)
        username=driver.find_element(By.XPATH,value='//input[@type="text"]')
        passwordname=driver.find_element(By.XPATH,value='//input[@type="password"]')
        login2=driver.find_element(By.XPATH,value='//button[@type="submit"]')

        username.send_keys(user)
        passwordname.send_keys(password)
        time.sleep(1)
        login2.click()
        time.sleep(3)
        p=driver.find_element(By.XPATH, "//div[@class='sidebar_inner']").find_elements(By.TAG_NAME, 'a')
        web2=p[3].get_attribute('href')
        driver.get(web2)
        c=driver.find_elements(By.TAG_NAME,'a')
        for num,i in enumerate(c):
            if num>=30:
                h=i.get_attribute('href')
                print(num,h)
    ```
- 7.6 开始爬虫
    ```python 
    scraper = WebScraper(option_s=False)
    time.sleep(5)
    scraper.login(user='*******',password='******')

    scraper.img_getting(choose_num=38,name='*****',scrolltimes=200)

    scraper.close()
    ```