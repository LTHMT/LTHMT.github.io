
## 1.爬虫相关概念

### 爬虫基本概念

#### 1.1 HTTP 请求 

- HTTP 请求是指客户端（通常是浏览器或爬虫）向服务器发送请求，以获取服务器上的资源。

- 常见的 HTTP 请求方法包括 `GET`（获取资源）、`POST`（提交数据）、`PUT`（更新资源）和 `DELETE`（删除资源）等。

- 通过发送 HTTP 请求，客户端可以获取网页的 HTML 文档并进一步处理和解析其中的数据。
    
#### 1.2 HTML 文档
    
- HTML 文档是描述网页结构和内容的文件。HTML 由各种标签（如 `<html>`、`<head>`、`<body>` 等）和文本内容组成。

- 这些标签用于定义网页的标题、段落、图片、链接等信息。浏览器解析 HTML 文档并呈现为用户可见的网页，而爬虫则解析 HTML 内容，以提取所需的数据。
    
#### 1.3 网页爬取 

- 网页爬取是指自动化程序从网页上获取数据的过程。爬虫通过发送 HTTP 请求获取 HTML 文档，并解析网页内容，从中提取出特定的信息，如标题、价格、评论等。通过网页爬取，可以高效地从大量网页中收集数据。
   

#### 1.4 网页爬取的一般流程 (ウェブスクレイピングの一般的なプロセス)
   
网页爬取通常包括以下几个步骤：

- **网页获取**：向目标网站发送 HTTP 请求，获取 HTML 文档。

- **网页解析**：使用解析工具（如正则表达式、XPath、CSS选择器）分析 HTML 文档，找到所需的内容位置。

- **内容提取**：从解析后的内容中提取目标数据。

- **数据存储**：将提取的数据存储到文件（如 CSV、JSON）或数据库中，以便后续分析和处理。

通过以上流程，爬虫能够系统化地从大量网页中提取所需数据，便于进一步分析和利用。

### 爬虫工具和库

#### 1. **Requests + BeautifulSoup（Python）**
   - **适用场景**：适用于静态网页的简单爬取，数据量不大，且页面结构简单的场景。
   - **特点**：易于学习和使用。`Requests`用于发送HTTP请求，`BeautifulSoup`用于解析HTML。
   - **优点**：轻量且灵活，可以快速上手进行小规模的数据抓取。
   - **缺点**：无法处理JavaScript渲染的内容，需要配合其他工具（如Selenium）来处理动态内容。

   ```python
   import requests
   from bs4 import BeautifulSoup

   url = 'https://example.com'
   response = requests.get(url)
   soup = BeautifulSoup(response.text, 'html.parser')
   # 使用soup对象查找页面内容
   ```

#### 2. **Scrapy（Python）**
   - **适用场景**：适用于中大型项目，且需要抓取大量页面或多层级页面的数据。
   - **特点**：一个强大的爬虫框架，支持异步抓取、自动处理请求队列，并且有丰富的扩展功能。
   - **优点**：结构化管理爬虫代码，支持多线程和异步请求，速度快且扩展性好。
   - **缺点**：学习曲线稍高，适合需要复杂抓取逻辑或长时间运行的项目。

   ```bash
   # 安装Scrapy
   pip install scrapy
   ```

   ```python
   # 使用Scrapy进行数据抓取需要定义Item、Spider、Pipeline等模块。
   scrapy startproject myproject
   cd myproject
   scrapy genspider example example.com
   ```

#### 3. **Selenium + BeautifulSoup（Python）**
   - **适用场景**：适用于动态网页内容抓取（例如需要执行JavaScript代码）。
   - **特点**：可以模拟浏览器操作，包括点击、滚动、等待等操作，适合爬取需要动态加载内容的网页。
   - **优点**：可以抓取动态内容，模拟用户交互。
   - **缺点**：速度较慢，资源消耗高，不适合大规模抓取。

   ```python
   from selenium import webdriver
   from bs4 import BeautifulSoup

   driver = webdriver.Chrome()
   driver.get('https://example.com')
   soup = BeautifulSoup(driver.page_source, 'html.parser')
   # 使用soup对象查找页面内容
   driver.quit()
   ```

#### 4. **Playwright（Python、Node.js）**
   - **适用场景**：适用于需要处理复杂动态内容和反爬虫机制较强的网站。
   - **特点**：可以模拟浏览器并行运行，支持多种编程语言，并且比Selenium更快。
   - **优点**：支持多浏览器，多标签页，高性能，适合需要模拟用户交互的场景。
   - **缺点**：稍微复杂，尤其是对Selenium用户来说有学习成本。

   ```bash
   # 安装Playwright
   pip install playwright
   playwright install
   ```

   ```python
   from playwright.sync_api import sync_playwright

   with sync_playwright() as p:
       browser = p.chromium.launch()
       page = browser.new_page()
       page.goto('https://example.com')
       html = page.content()
       browser.close()
   ```

#### 5. **Headless Chrome（如 Puppeteer 或 Pyppeteer）**
   - **适用场景**：适用于需要JavaScript渲染和用户模拟的情况。
   - **特点**：一个专门用来控制无头Chrome浏览器的库，适合处理复杂的动态页面。
   - **优点**：提供浏览器级的控制，支持JavaScript渲染。
   - **缺点**：需要一定的学习成本，性能相对较慢。

   ```javascript
   // 使用Node.js中的Puppeteer
   const puppeteer = require('puppeteer');

   (async () => {
       const browser = await puppeteer.launch();
       const page = await browser.newPage();
       await page.goto('https://example.com');
       const content = await page.content();
       await browser.close();
   })();
   ```

#### 选择依据
- **静态页面**：`Requests + BeautifulSoup` 或 `Scrapy` 是不错的选择。
- **动态页面（需要JS渲染）**：可以使用 `Selenium`、`Playwright`、或 `Puppeteer/Pyppeteer`。
- **大规模抓取**：`Scrapy` 更适合处理大规模抓取任务。
- **复杂的用户交互**：`Selenium` 或 `Playwright` 是更好的选择。

根据你的项目需求、性能要求、页面类型等选择适合的工具。


## 2. 基础爬虫工具：Requests 和 BeautifulSoup

在 Python 中，可以使用 `requests` 库发送 HTTP 请求，用 `BeautifulSoup` 库解析 HTML 文档。以下是一个简单示例：

```python
import requests
from bs4 import BeautifulSoup

# 发送请求
url = "https://example.com"
response = requests.get(url)

# 解析 HTML 内容
soup = BeautifulSoup(response.text, 'html.parser')

# 提取内容，例如标题
title = soup.find('title').text
print(f"网页标题: {title}")
```

## 3. 动态网页和 Selenium 的应用

一些网页内容是通过 JavaScript 动态加载的，传统的爬虫方法无法直接获取。为此，我们可以使用 `Selenium`，这是一个自动化浏览器工具，可以模拟用户操作并执行 JavaScript 代码，从而加载网页中的动态内容。

### 3.1 安装 Selenium 和浏览器驱动

首先，安装 Selenium：
```bash
pip install selenium
```

然后，下载对应的浏览器驱动（如 ChromeDriver）并将其路径添加到系统环境变量中。

### 3.2 使用 Selenium 获取动态内容

以下是一个使用 Selenium 抓取动态内容的示例：

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
import time

# 初始化 Chrome 浏览器
driver = webdriver.Chrome()

# 打开网页
url = "https://example.com"
driver.get(url)

# 等待页面加载完成
time.sleep(3)

# 提取动态内容，例如获取带有特定类名的元素
elements = driver.find_elements(By.CLASS_NAME, "dynamic-content")
for element in elements:
    print(element.text)

# 关闭浏览器
driver.quit()
```

### 3.3 Selenium 操作网页元素

Selenium 提供了丰富的 API，可以实现模拟用户操作，如点击按钮、输入文本等。

- **输入文本**：
  ```python
  input_box = driver.find_element(By.ID, "search-box")
  input_box.send_keys("Selenium 爬虫")
  ```

- **点击按钮**：
  ```python
  search_button = driver.find_element(By.ID, "search-button")
  search_button.click()
  ```

- **滚动页面**（加载更多内容）：
  ```python
  driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
  ```


!> **避免过快请求**：添加适当的 `time.sleep()` 延迟，避免频繁请求服务器。

!> **遵循 robots.txt**：查看网站的 `robots.txt` 文件，确保爬虫行为不违反网站规定。

!> **错误处理**：使用 `try-except` 捕获错误，确保爬虫在遇到异常时不会崩溃。

## 4. 完整示例：爬取动态内容

以下是一个完整的示例，使用 Selenium 模拟用户搜索内容并抓取结果：

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
import time

# 初始化 Chrome 浏览器
driver = webdriver.Chrome()

try:
    # 打开目标网页
    url = "https://example.com/search"
    driver.get(url)

    # 输入搜索关键词
    search_box = driver.find_element(By.ID, "search-box")
    search_box.send_keys("Python 爬虫")

    # 点击搜索按钮
    search_button = driver.find_element(By.ID, "search-button")
    search_button.click()

    # 等待加载搜索结果
    time.sleep(3)

    # 提取结果
    results = driver.find_elements(By.CLASS_NAME, "result-title")
    for result in results:
        print(result.text)

finally:
    # 关闭浏览器
    driver.quit()
```

## 6. 数据存储

可以将爬取的数据存储为 CSV 或 JSON 文件：

```python
import csv

# 存储为 CSV
with open('data.csv', mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(['Title'])  # 表头
    for result in results:
        writer.writerow([result.text])
```

---

## 7.个人selenium爬虫实操笔记
?>此为没有加密的网站的动态爬虫

- 7.1 导入包
    ```python 
    from selenium import webdriver
    import requests
    from selenium.webdriver.common.by import By
    import os
    from pathlib import Path
    from tqdm import tqdm
    import time
    from tqdm.contrib import tenumerate,tzip,tmap
    ```

- 7.2 定义保存图片函数
    ```python
    def get_img(url, fn):
    # Get the user's home directory
        home_dir = str(Path.home())
    
    # Create the 'ssunbiki' directory in the home directory
        os.makedirs(os.path.join(home_dir, 'hana-bunny'), exist_ok=True)
    
    # Download the image and save it to the 'ssunbiki' directory
        resp = requests.get(url)
        with open(os.path.join(home_dir, 'hana-bunny', '{}.png'.format(fn)), 'wb') as f:
            f.write(resp.content)

    ```

- 7.3 配置webdriver
    ```python
    options=webdriver.ChromeOptions()
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--sandbox')
    #options.add_argument('--headless')
    ```

- 7.4 定义网站登录器
    ```python
    def login(url,user,password):
        driver=webdriver.Chrome(options=options)
        driver.get(url)
        driver.set_window_size(1920, 1080)
        #user=str(user)
        #password=str(password)
        username=driver.find_element(By.XPATH,value='//input[@type="text"]')
        passwordname=driver.find_element(By.XPATH,value='//input[@type="password"]')
        login2=driver.find_element(By.XPATH,value='//button[@type="submit"]')

        username.send_keys(user)
        passwordname.send_keys(password)
        time.sleep(1)
        login2.click()
        time.sleep(3)
        p=driver.find_element(By.XPATH, "//div[@class='sidebar_inner']").find_elements(By.TAG_NAME, 'a')
        web2=p[3].get_attribute('href')
        driver.get(web2)
        c=driver.find_elements(By.TAG_NAME,'a')
        for num,i in enumerate(c):
            if num>=30:
                h=i.get_attribute('href')
                print(num,h)
    ```

- 7.5 定义图片获取
    ```python
    def login(url,user,password):
        driver=webdriver.Chrome(options=options)
        driver.get(url)
        driver.set_window_size(1920, 1080)
        #user=str(user)
        #password=str(password)
        username=driver.find_element(By.XPATH,value='//input[@type="text"]')
        passwordname=driver.find_element(By.XPATH,value='//input[@type="password"]')
        login2=driver.find_element(By.XPATH,value='//button[@type="submit"]')

        username.send_keys(user)
        passwordname.send_keys(password)
        time.sleep(1)
        login2.click()
        time.sleep(3)
        p=driver.find_element(By.XPATH, "//div[@class='sidebar_inner']").find_elements(By.TAG_NAME, 'a')
        web2=p[3].get_attribute('href')
        driver.get(web2)
        c=driver.find_elements(By.TAG_NAME,'a')
        for num,i in enumerate(c):
            if num>=30:
                h=i.get_attribute('href')
                print(num,h)
    ```
- 7.6 开始爬虫
    ```python 
    scraper = WebScraper(option_s=False)
    time.sleep(5)
    scraper.login(user='*******',password='******')

    scraper.img_getting(choose_num=38,name='*****',scrolltimes=200)

    scraper.close()
    ```